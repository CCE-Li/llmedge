/*
 * Copyright (C) 2024 LLMEdge Contributors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package io.aatricks.llmedge

import android.content.Context
import android.os.Build
import android.util.Log
import io.aatricks.llmedge.huggingface.HuggingFaceHub
import java.io.File
import java.io.FileNotFoundException
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.flow.Flow
import kotlinx.coroutines.flow.flow
import kotlinx.coroutines.flow.flowOn
import kotlinx.coroutines.withContext

/**
 * Kotlin wrapper for whisper.cpp providing Speech-to-Text (STT) functionality.
 * 
 * Whisper is an automatic speech recognition (ASR) model that can transcribe
 * and translate audio in multiple languages. This wrapper enables:
 * - Real-time transcription
 * - Translation between languages  
 * - Subtitle generation with timestamps
 * - Language detection
 * 
 * Example usage:
 * ```kotlin
 * val whisper = Whisper.load(context, "path/to/ggml-base.bin")
 * 
 * // Transcribe audio samples (16kHz mono PCM float32)
 * val segments = whisper.transcribe(audioSamples)
 * segments.forEach { segment ->
 *     println("[${segment.startTimeMs}ms - ${segment.endTimeMs}ms] ${segment.text}")
 * }
 * 
 * whisper.close()
 * ```
 */
class Whisper private constructor(private val handle: Long) : AutoCloseable {
    
    /**
     * Represents a transcribed segment with timing information.
     */
    data class TranscriptionSegment(
        val index: Int,
        val startTime: Long,  // Start time in whisper time units (centiseconds)
        val endTime: Long,    // End time in whisper time units (centiseconds)
        val text: String
    ) {
        /** Start time in milliseconds */
        val startTimeMs: Long get() = startTime * 10
        
        /** End time in milliseconds */
        val endTimeMs: Long get() = endTime * 10
        
        /** Duration in milliseconds */
        val durationMs: Long get() = endTimeMs - startTimeMs
        
        /** Format as SRT subtitle entry */
        fun toSrtEntry(): String {
            val startFormatted = formatTimeSrt(startTimeMs)
            val endFormatted = formatTimeSrt(endTimeMs)
            return "${index + 1}\n$startFormatted --> $endFormatted\n$text\n"
        }
        
        /** Format as VTT subtitle entry */
        fun toVttEntry(): String {
            val startFormatted = formatTimeVtt(startTimeMs)
            val endFormatted = formatTimeVtt(endTimeMs)
            return "$startFormatted --> $endFormatted\n$text\n"
        }
        
        private fun formatTimeSrt(ms: Long): String {
            val hours = ms / 3600000
            val minutes = (ms % 3600000) / 60000
            val seconds = (ms % 60000) / 1000
            val millis = ms % 1000
            return String.format("%02d:%02d:%02d,%03d", hours, minutes, seconds, millis)
        }
        
        private fun formatTimeVtt(ms: Long): String {
            val hours = ms / 3600000
            val minutes = (ms % 3600000) / 60000
            val seconds = (ms % 60000) / 1000
            val millis = ms % 1000
            return String.format("%02d:%02d:%02d.%03d", hours, minutes, seconds, millis)
        }
    }
    
    /**
     * Configuration for transcription.
     */
    data class TranscribeParams(
        /** Number of threads to use. 0 = auto */
        val nThreads: Int = 0,
        /** Translate to English instead of transcribing */
        val translate: Boolean = false,
        /** Target language code (e.g., "en", "es", "fr"). null = auto-detect */
        val language: String? = null,
        /** Force language detection even if language is specified */
        val detectLanguage: Boolean = false,
        /** Enable token-level timestamps for more precise timing */
        val tokenTimestamps: Boolean = false,
        /** Maximum segment length in characters. 0 = no limit */
        val maxLen: Int = 0,
        /** Split on word boundaries when using maxLen */
        val splitOnWord: Boolean = false,
        /** Sampling temperature (0.0 = greedy, higher = more random) */
        val temperature: Float = 0.0f,
        /** Beam size for beam search (1 = greedy decoding) */
        val beamSize: Int = 1,
        /** Suppress blank tokens at the beginning of segments */
        val suppressBlank: Boolean = true,
        /** Print progress to console */
        val printProgress: Boolean = false
    )
    
    /**
     * Callback for transcription progress updates.
     */
    fun interface ProgressCallback {
        fun onProgress(progress: Int)
    }
    
    /**
     * Callback for new transcription segments (for real-time streaming).
     */
    fun interface SegmentCallback {
        fun onNewSegment(index: Int, startTime: Long, endTime: Long, text: String)
    }
    
    private var progressCallback: ProgressCallback? = null
    private var segmentCallback: SegmentCallback? = null
    
    // Internal bridge interface for testing
    internal interface NativeBridge {
        fun transcribe(
            handle: Long,
            samples: FloatArray,
            params: TranscribeParams,
            progressCallback: ProgressCallback?,
            segmentCallback: SegmentCallback?
        ): Array<TranscriptionSegment>?
        
        fun detectLanguage(handle: Long, samples: FloatArray, nThreads: Int): Int
        fun getFullText(handle: Long): String
        fun close(handle: Long)
    }
    
    /**
     * Set a callback to receive progress updates during transcription.
     */
    fun setProgressCallback(callback: ProgressCallback?) {
        this.progressCallback = callback
        if (callback != null) {
            nativeSetProgressCallback(handle, object : Any() {
                @Suppress("unused")
                fun onProgress(progress: Int) {
                    callback.onProgress(progress)
                }
            })
        } else {
            nativeSetProgressCallback(handle, null)
        }
    }
    
    /**
     * Set a callback to receive new segments in real-time during transcription.
     * This is useful for streaming transcription results.
     */
    fun setSegmentCallback(callback: SegmentCallback?) {
        this.segmentCallback = callback
        if (callback != null) {
            nativeSetSegmentCallback(handle, object : Any() {
                @Suppress("unused")
                fun onNewSegment(index: Int, startTime: Long, endTime: Long, text: String) {
                    callback.onNewSegment(index, startTime, endTime, text)
                }
            })
        } else {
            nativeSetSegmentCallback(handle, null)
        }
    }
    
    /**
     * Transcribe audio samples to text.
     * 
     * @param samples Audio samples as 32-bit float PCM at 16kHz mono
     * @param params Transcription parameters
     * @return List of transcription segments with timing information
     */
    fun transcribe(
        samples: FloatArray,
        params: TranscribeParams = TranscribeParams()
    ): List<TranscriptionSegment> {
        require(samples.isNotEmpty()) { "Audio samples cannot be empty" }
        
        val effectiveThreads = if (params.nThreads <= 0) {
            Runtime.getRuntime().availableProcessors().coerceAtMost(8)
        } else {
            params.nThreads
        }
        
        val segments = nativeTranscribe(
            handle,
            samples,
            effectiveThreads,
            params.translate,
            params.language,
            params.detectLanguage,
            params.tokenTimestamps,
            params.maxLen,
            params.splitOnWord,
            params.temperature,
            params.beamSize,
            params.suppressBlank,
            params.printProgress
        ) ?: return emptyList()
        
        return segments.toList()
    }
    
    /**
     * Transcribe audio and return results as a Flow for streaming use cases.
     */
    fun transcribeFlow(
        samples: FloatArray,
        params: TranscribeParams = TranscribeParams()
    ): Flow<TranscriptionSegment> = flow {
        val segments = transcribe(samples, params)
        segments.forEach { emit(it) }
    }.flowOn(Dispatchers.IO)
    
    /**
     * Detect the language of the audio.
     * 
     * @param samples Audio samples as 32-bit float PCM at 16kHz mono
     * @param nThreads Number of threads to use. 0 = auto
     * @return Language code (e.g., "en", "es", "fr") or null if detection fails
     */
    fun detectLanguage(samples: FloatArray, nThreads: Int = 0): String? {
        require(samples.isNotEmpty()) { "Audio samples cannot be empty" }
        
        val effectiveThreads = if (nThreads <= 0) {
            Runtime.getRuntime().availableProcessors().coerceAtMost(8)
        } else {
            nThreads
        }
        
        val langId = nativeDetectLanguage(handle, samples, effectiveThreads, 0)
        return if (langId >= 0) getLanguageString(langId) else null
    }
    
    /**
     * Get the full transcribed text from the last transcription.
     */
    fun getFullText(): String = nativeGetFullText(handle)
    
    /**
     * Check if the loaded model supports multiple languages.
     */
    fun isMultilingual(): Boolean = nativeIsMultilingual(handle)
    
    /**
     * Get the model type (e.g., "tiny", "base", "small", "medium", "large").
     */
    fun getModelType(): String = nativeGetModelType(handle)
    
    /**
     * Reset internal timing statistics.
     */
    fun resetTimings() = nativeResetTimings(handle)
    
    /**
     * Print timing statistics to the log.
     */
    fun printTimings() = nativePrintTimings(handle)
    
    /**
     * Generate SRT subtitle content from transcription segments.
     */
    fun generateSrt(segments: List<TranscriptionSegment>): String {
        return segments.joinToString("\n") { it.toSrtEntry() }
    }
    
    /**
     * Generate WebVTT subtitle content from transcription segments.
     */
    fun generateVtt(segments: List<TranscriptionSegment>): String {
        val header = "WEBVTT\n\n"
        return header + segments.joinToString("\n") { it.toVttEntry() }
    }
    
    override fun close() {
        nativeDestroy(handle)
    }
    
    companion object {
        private const val LOG_TAG = "Whisper"
        
        /** Whisper expects audio at 16kHz sample rate */
        const val SAMPLE_RATE = 16000
        
        /** Whisper processes audio in 30-second chunks */
        const val CHUNK_SIZE_SECONDS = 30
        
        private val isAndroidLogAvailable: Boolean = try {
            Class.forName("android.util.Log")
            true
        } catch (_: Throwable) {
            false
        }
        
        private fun logD(tag: String, message: String) {
            if (isAndroidLogAvailable) {
                try {
                    Log.d(tag, message)
                } catch (t: Throwable) {
                    println("D/$tag: $message")
                }
            } else {
                println("D/$tag: $message")
            }
        }
        
        private fun logE(tag: String, message: String) {
            if (isAndroidLogAvailable) {
                try {
                    Log.e(tag, message)
                } catch (t: Throwable) {
                    println("E/$tag: $message")
                }
            } else {
                println("E/$tag: $message")
            }
        }
        
        // Native library loading - similar to SmolLM
        init {
            val disableNativeLoad = java.lang.Boolean.getBoolean("llmedge.disableNativeLoad")
            if (disableNativeLoad) {
                println("[Whisper] Native library load disabled via llmedge.disableNativeLoad=true")
            } else {
                try {
                    // Try to load the whisper native library
                    // On Android, we have architecture-specific variants
                    // On desktop/JVM testing, we use whisper_jni
                    
                    // First, check if this is a desktop JVM environment (for testing)
                    val osName = System.getProperty("os.name", "").lowercase()
                    val isDesktopJvm = osName.contains("linux") && !osName.contains("android")
                    
                    if (isDesktopJvm) {
                        // Desktop JVM testing - load whisper_jni directly
                        logD(LOG_TAG, "Loading libwhisper_jni.so for desktop testing")
                        System.loadLibrary("whisper_jni")
                    } else {
                        // Android environment
                        val isEmulated = Build.HARDWARE.contains("goldfish") || Build.HARDWARE.contains("ranchu")
                        
                        if (!isEmulated && supportsArm64V8a()) {
                            logD(LOG_TAG, "Loading libwhisper_arm64.so")
                            System.loadLibrary("whisper_arm64")
                        } else {
                            logD(LOG_TAG, "Loading default libwhisper.so")
                            System.loadLibrary("whisper")
                        }
                    }
                } catch (e: UnsatisfiedLinkError) {
                    logE(LOG_TAG, "Failed to load whisper native library: ${e.message}")
                }
            }
        }
        
        private fun supportsArm64V8a(): Boolean {
            return Build.SUPPORTED_64_BIT_ABIS.any { it == "arm64-v8a" }
        }
        
        /**
         * Check if native bindings are available.
         */
        @JvmStatic
        fun checkBindings(): Boolean {
            return try {
                nativeCheckBindings()
            } catch (e: UnsatisfiedLinkError) {
                false
            }
        }
        
        /**
         * Get the whisper.cpp version string.
         */
        @JvmStatic
        fun getVersion(): String = nativeGetVersion()
        
        /**
         * Get system information string.
         */
        @JvmStatic
        fun getSystemInfo(): String = nativeGetSystemInfo()
        
        /**
         * Get the maximum language ID supported.
         */
        @JvmStatic
        fun getMaxLanguageId(): Int = nativeGetMaxLanguageId()
        
        /**
         * Get the language ID for a language code or name.
         * 
         * @param lang Language code (e.g., "en") or name (e.g., "english")
         * @return Language ID or -1 if not found
         */
        @JvmStatic
        fun getLanguageId(lang: String): Int = nativeGetLanguageId(lang)
        
        /**
         * Get the language code for a language ID.
         * 
         * @param langId Language ID
         * @return Language code (e.g., "en") or empty string if not found
         */
        @JvmStatic
        fun getLanguageString(langId: Int): String = nativeGetLanguageString(langId)
        
        /**
         * Load a Whisper model from a file path.
         * 
         * @param modelPath Path to the GGML Whisper model file
         * @param useGpu Enable GPU acceleration (if available)
         * @param flashAttn Enable flash attention optimization
         * @param gpuDevice GPU device index (for multi-GPU systems)
         * @return Whisper instance
         */
        @JvmStatic
        fun load(
            modelPath: String,
            useGpu: Boolean = false,
            flashAttn: Boolean = true,
            gpuDevice: Int = 0
        ): Whisper {
            val file = File(modelPath)
            if (!file.exists()) {
                throw FileNotFoundException("Model file not found: $modelPath")
            }
            
            val handle = nativeCreate(modelPath, useGpu, flashAttn, gpuDevice)
            if (handle == 0L) {
                throw RuntimeException("Failed to load Whisper model from: $modelPath")
            }
            
            return Whisper(handle)
        }
        
        /**
         * Load a Whisper model with Android Context support.
         * This allows loading models from app assets or cache directories.
         * 
         * @param context Android context
         * @param modelPath Path to the model file (can be relative to cache dir)
         * @param useGpu Enable GPU acceleration
         * @param flashAttn Enable flash attention optimization
         * @param gpuDevice GPU device index
         * @return Whisper instance
         */
        @JvmStatic
        suspend fun load(
            context: Context,
            modelPath: String,
            useGpu: Boolean = false,
            flashAttn: Boolean = true,
            gpuDevice: Int = 0
        ): Whisper = withContext(Dispatchers.IO) {
            val file = File(modelPath)
            val actualPath = if (file.exists()) {
                modelPath
            } else {
                // Try to find in cache directory
                val cacheFile = File(context.cacheDir, modelPath)
                if (cacheFile.exists()) {
                    cacheFile.absolutePath
                } else {
                    // Try to find in files directory
                    val filesFile = File(context.filesDir, modelPath)
                    if (filesFile.exists()) {
                        filesFile.absolutePath
                    } else {
                        throw FileNotFoundException("Model file not found: $modelPath")
                    }
                }
            }
            
            load(actualPath, useGpu, flashAttn, gpuDevice)
        }
        
        /**
         * Download and load a Whisper model from Hugging Face Hub.
         * 
         * @param context Android context for caching
         * @param modelId Hugging Face model ID (e.g., "ggerganov/whisper.cpp")
         * @param modelFile Specific model file name (e.g., "ggml-base.bin")
         * @param useGpu Enable GPU acceleration
         * @param flashAttn Enable flash attention optimization
         * @param gpuDevice GPU device index
         * @param token Optional Hugging Face API token for private models
         * @return Whisper instance
         */
        @JvmStatic
        suspend fun loadFromHuggingFace(
            context: Context,
            modelId: String = "ggerganov/whisper.cpp",
            modelFile: String = "ggml-base.bin",
            useGpu: Boolean = false,
            flashAttn: Boolean = true,
            gpuDevice: Int = 0,
            token: String? = null
        ): Whisper = withContext(Dispatchers.IO) {
            val result = HuggingFaceHub.ensureModelOnDisk(
                context = context,
                modelId = modelId,
                filename = modelFile,
                token = token
            )
            load(result.file.absolutePath, useGpu, flashAttn, gpuDevice)
        }
        
        // Native method declarations
        @JvmStatic private external fun nativeCheckBindings(): Boolean
        @JvmStatic private external fun nativeGetVersion(): String
        @JvmStatic private external fun nativeGetSystemInfo(): String
        @JvmStatic private external fun nativeGetMaxLanguageId(): Int
        @JvmStatic private external fun nativeGetLanguageId(lang: String): Int
        @JvmStatic private external fun nativeGetLanguageString(langId: Int): String
        @JvmStatic private external fun nativeCreate(
            modelPath: String,
            useGpu: Boolean,
            flashAttn: Boolean,
            gpuDevice: Int
        ): Long
        @JvmStatic private external fun nativeDestroy(handle: Long)
        @JvmStatic private external fun nativeIsMultilingual(handle: Long): Boolean
        @JvmStatic private external fun nativeGetModelType(handle: Long): String
        @JvmStatic private external fun nativeSetProgressCallback(handle: Long, callback: Any?)
        @JvmStatic private external fun nativeSetSegmentCallback(handle: Long, callback: Any?)
        @JvmStatic private external fun nativeTranscribe(
            handle: Long,
            samples: FloatArray,
            nThreads: Int,
            translate: Boolean,
            language: String?,
            detectLanguage: Boolean,
            tokenTimestamps: Boolean,
            maxLen: Int,
            splitOnWord: Boolean,
            temperature: Float,
            beamSize: Int,
            suppressBlank: Boolean,
            printProgress: Boolean
        ): Array<TranscriptionSegment>?
        @JvmStatic private external fun nativeDetectLanguage(
            handle: Long,
            samples: FloatArray,
            nThreads: Int,
            offsetMs: Int
        ): Int
        @JvmStatic private external fun nativeGetFullText(handle: Long): String
        @JvmStatic private external fun nativeResetTimings(handle: Long)
        @JvmStatic private external fun nativePrintTimings(handle: Long)
    }
}
