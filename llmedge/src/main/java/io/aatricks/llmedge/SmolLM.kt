/*
 * Copyright (C) 2024 Shubham Panchal
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package io.aatricks.llmedge

import android.content.Context
import android.os.Build
import android.util.Log
import io.aatricks.llmedge.huggingface.HuggingFaceHub
import java.io.File
import java.io.FileNotFoundException
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.flow.Flow
import kotlinx.coroutines.flow.flow
import kotlinx.coroutines.flow.flowOn
import kotlinx.coroutines.withContext

/**
 * Kotlin wrapper for the native LLM runtime. Handles loading models and providing a simple API for
 * running completions and managing model state.
 */
class SmolLM(useVulkan: Boolean = true) : AutoCloseable {

    internal interface NativeBridge {
        fun loadModel(
                instance: SmolLM,
                modelPath: String,
                minP: Float,
                temperature: Float,
                storeChats: Boolean,
                contextSize: Long,
                chatTemplate: String,
                nThreads: Int,
                useMmap: Boolean,
                useMlock: Boolean,
                useVulkan: Boolean,
        ): Long

        fun setReasoningOptions(
                instance: SmolLM,
                modelPtr: Long,
                disableThinking: Boolean,
                reasoningBudget: Int
        )
        fun addChatMessage(instance: SmolLM, modelPtr: Long, message: String, role: String)
        fun getResponseGenerationSpeed(instance: SmolLM, modelPtr: Long): Float
        fun getResponseGeneratedTokenCount(instance: SmolLM, modelPtr: Long): Long
        fun getResponseGenerationDurationMicros(instance: SmolLM, modelPtr: Long): Long
        fun getContextSizeUsed(instance: SmolLM, modelPtr: Long): Int
        fun getNativeModelPtr(instance: SmolLM, modelPtr: Long): Long
        fun nativeDecodePreparedEmbeddings(
                instance: SmolLM,
                modelPtr: Long,
                embdPath: String,
                metaPath: String,
                nBatch: Int
        ): Boolean
        fun close(instance: SmolLM, modelPtr: Long)
        fun startCompletion(instance: SmolLM, modelPtr: Long, prompt: String)
        fun completionLoop(instance: SmolLM, modelPtr: Long): String
        fun stopCompletion(instance: SmolLM, modelPtr: Long)
    }
    companion object {
        private const val LOG_TAG = "SmolLM"
        private const val DEFAULT_CONTEXT_SIZE_CAP: Long = 8_192L
        private const val MIN_CONTEXT_SIZE: Long = 1_024L
        private const val DEFAULT_REASONING_BUDGET: Int = -1

        private val isAndroidLogAvailable: Boolean =
                try {
                    Class.forName("android.util.Log")
                    true
                } catch (_: Throwable) {
                    false
                }

        private fun logD(tag: String, message: String) {
            if (isAndroidLogAvailable) {
                try {
                    val logClass = Class.forName("android.util.Log")
                    val dMethod = logClass.getMethod("d", String::class.java, String::class.java)
                    dMethod.invoke(null, tag, message)
                } catch (t: Throwable) {
                    println("D/$tag: $message")
                }
            } else {
                println("D/$tag: $message")
            }
        }

        private fun logW(tag: String, message: String) {
            if (isAndroidLogAvailable) {
                try {
                    val logClass = Class.forName("android.util.Log")
                    val wMethod = logClass.getMethod("w", String::class.java, String::class.java)
                    wMethod.invoke(null, tag, message)
                } catch (t: Throwable) {
                    println("W/$tag: $message")
                }
            } else {
                println("W/$tag: $message")
            }
        }

        init {
            val logTag = LOG_TAG

            val disableNativeLoad = java.lang.Boolean.getBoolean("llmedge.disableNativeLoad")
            if (disableNativeLoad) {
                println("[SmolLM] Native library load disabled via llmedge.disableNativeLoad=true")
            } else {
                // check if the following CPU features are available,
                // and load the native library accordingly
                val cpuFeatures = getCPUFeatures()
                val hasFp16 = cpuFeatures.contains("fp16") || cpuFeatures.contains("fphp")
                val hasDotProd = cpuFeatures.contains("dotprod") || cpuFeatures.contains("asimddp")
                val hasSve = cpuFeatures.contains("sve")
                val hasI8mm = cpuFeatures.contains("i8mm")
                val isAtLeastArmV82 =
                        cpuFeatures.contains("asimd") &&
                                cpuFeatures.contains("crc32") &&
                                cpuFeatures.contains(
                                        "aes",
                                )
                val isAtLeastArmV84 = cpuFeatures.contains("dcpop") && cpuFeatures.contains("uscat")

                logD(logTag, "CPU features: $cpuFeatures")
                logD(logTag, "- hasFp16: $hasFp16")
                logD(logTag, "- hasDotProd: $hasDotProd")
                logD(logTag, "- hasSve: $hasSve")
                logD(logTag, "- hasI8mm: $hasI8mm")
                logD(logTag, "- isAtLeastArmV82: $isAtLeastArmV82")
                logD(logTag, "- isAtLeastArmV84: $isAtLeastArmV84")

                // Check if the app is running in an emulated device
                // Note, this is not the OFFICIAL way to check if the app is running
                // on an emulator
                val isEmulated =
                        (Build.HARDWARE.contains("goldfish") || Build.HARDWARE.contains("ranchu"))
                logD(logTag, "isEmulated: $isEmulated")

                if (!isEmulated) {
                    if (supportsArm64V8a()) {
                        if (isAtLeastArmV84 && hasSve && hasI8mm && hasFp16 && hasDotProd) {
                            Log.d(logTag, "Loading libsmollm_v8_4_fp16_dotprod_i8mm_sve.so")
                            System.loadLibrary("smollm_v8_4_fp16_dotprod_i8mm_sve")
                        } else if (isAtLeastArmV84 && hasSve && hasFp16 && hasDotProd) {
                            Log.d(logTag, "Loading libsmollm_v8_4_fp16_dotprod_sve.so")
                            System.loadLibrary("smollm_v8_4_fp16_dotprod_sve")
                        } else if (isAtLeastArmV84 && hasI8mm && hasFp16 && hasDotProd) {
                            Log.d(logTag, "Loading libsmollm_v8_4_fp16_dotprod_i8mm.so")
                            System.loadLibrary("smollm_v8_4_fp16_dotprod_i8mm")
                        } else if (isAtLeastArmV84 && hasFp16 && hasDotProd) {
                            Log.d(logTag, "Loading libsmollm_v8_4_fp16_dotprod.so")
                            System.loadLibrary("smollm_v8_4_fp16_dotprod")
                        } else if (isAtLeastArmV82 && hasFp16 && hasDotProd) {
                            Log.d(logTag, "Loading libsmollm_v8_2_fp16_dotprod.so")
                            System.loadLibrary("smollm_v8_2_fp16_dotprod")
                        } else if (isAtLeastArmV82 && hasFp16) {
                            Log.d(logTag, "Loading libsmollm_v8_2_fp16.so")
                            System.loadLibrary("smollm_v8_2_fp16")
                        } else {
                            Log.d(logTag, "Loading libsmollm_v8.so")
                            System.loadLibrary("smollm_v8")
                        }
                    } else if (Build.SUPPORTED_32_BIT_ABIS[0]?.equals("armeabi-v7a") == true) {
                        // armv7a (32bit) device
                        Log.d(logTag, "Loading libsmollm_v7a.so")
                        System.loadLibrary("smollm_v7a")
                    } else {
                        Log.d(logTag, "Loading default libsmollm.so")
                        System.loadLibrary("smollm")
                    }
                } else {
                    // load the default native library with no ARM
                    // specific instructions
                    Log.d(logTag, "Loading default libsmollm.so")
                    System.loadLibrary("smollm")
                }
            }
        }

        /**
         * Reads the /proc/cpuinfo file and returns the line starting with 'Features :' that
         * containing the available CPU features
         */
        private fun getCPUFeatures(): String {
            val cpuInfo =
                    try {
                        File("/proc/cpuinfo").readText()
                    } catch (e: FileNotFoundException) {
                        ""
                    }
            val cpuFeatures =
                    cpuInfo.substringAfter("Features")
                            .substringAfter(":")
                            .substringBefore("\n")
                            .trim()
            return cpuFeatures
        }

        private fun supportsArm64V8a(): Boolean = Build.SUPPORTED_ABIS[0].equals("arm64-v8a")

        private val defaultNativeBridgeProvider: (SmolLM) -> NativeBridge = { instance ->
            object : NativeBridge {
                override fun loadModel(
                        instance: SmolLM,
                        modelPath: String,
                        minP: Float,
                        temperature: Float,
                        storeChats: Boolean,
                        contextSize: Long,
                        chatTemplate: String,
                        nThreads: Int,
                        useMmap: Boolean,
                        useMlock: Boolean,
                        useVulkan: Boolean,
                ): Long =
                        instance.loadModel(
                                modelPath,
                                minP,
                                temperature,
                                storeChats,
                                contextSize,
                                chatTemplate,
                                nThreads,
                                useMmap,
                                useMlock,
                                useVulkan
                        )

                override fun setReasoningOptions(
                        instance: SmolLM,
                        modelPtr: Long,
                        disableThinking: Boolean,
                        reasoningBudget: Int
                ) = instance.setReasoningOptions(modelPtr, disableThinking, reasoningBudget)

                override fun addChatMessage(
                        instance: SmolLM,
                        modelPtr: Long,
                        message: String,
                        role: String
                ) = instance.addChatMessage(modelPtr, message, role)

                override fun getResponseGenerationSpeed(instance: SmolLM, modelPtr: Long): Float =
                        instance.getResponseGenerationSpeed(modelPtr)
                override fun getResponseGeneratedTokenCount(
                        instance: SmolLM,
                        modelPtr: Long
                ): Long = instance.getResponseGeneratedTokenCount(modelPtr)
                override fun getResponseGenerationDurationMicros(
                        instance: SmolLM,
                        modelPtr: Long
                ): Long = instance.getResponseGenerationDurationMicros(modelPtr)
                override fun getContextSizeUsed(instance: SmolLM, modelPtr: Long): Int =
                        instance.getContextSizeUsed(modelPtr)
                override fun getNativeModelPtr(instance: SmolLM, modelPtr: Long): Long =
                        instance.getNativeModelPtr(modelPtr)
                override fun nativeDecodePreparedEmbeddings(
                        instance: SmolLM,
                        modelPtr: Long,
                        embdPath: String,
                        metaPath: String,
                        nBatch: Int
                ): Boolean =
                        instance.nativeDecodePreparedEmbeddings(
                                modelPtr,
                                embdPath,
                                metaPath,
                                nBatch
                        )
                override fun close(instance: SmolLM, modelPtr: Long) = instance.close(modelPtr)
                override fun startCompletion(instance: SmolLM, modelPtr: Long, prompt: String) =
                        instance.startCompletion(modelPtr, prompt)
                override fun completionLoop(instance: SmolLM, modelPtr: Long): String =
                        instance.completionLoop(modelPtr)
                override fun stopCompletion(instance: SmolLM, modelPtr: Long) =
                        instance.stopCompletion(modelPtr)
            }
        }

        @Volatile
        private var nativeBridgeProvider: (SmolLM) -> NativeBridge = defaultNativeBridgeProvider

        internal fun overrideNativeBridgeForTests(provider: (SmolLM) -> NativeBridge) {
            nativeBridgeProvider = provider
        }

        internal fun resetNativeBridgeForTests() {
            nativeBridgeProvider = defaultNativeBridgeProvider
        }

        internal fun createLoadedForTests(nativePtr: Long, useVulkan: Boolean = false): SmolLM {
            val s = SmolLM(useVulkan)
            s.nativePtr = nativePtr
            return s
        }
    }

    private var nativePtr = 0L
    private val nativeBridge: NativeBridge = Companion.nativeBridgeProvider(this)
    private var useVulkanGPU = true
    private var currentThinkingMode = ThinkingMode.DEFAULT
    private var currentReasoningBudget = DEFAULT_REASONING_BUDGET

    init {
        this.useVulkanGPU = useVulkan
    }

    /** Returns true if this SmolLM instance will try to use Vulkan-backed GPU layers. */
    fun isVulkanEnabled(): Boolean = useVulkanGPU

    /**
     * Provides default values for inference parameters. These values are used when the
     * corresponding parameters are not provided by the user or are not available in the GGUF model
     * file.
     */
    object DefaultInferenceParams {
        val contextSize: Long = 1024L
        val chatTemplate: String =
                "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|> ' }}{% endif %}{{'<|im_start|>' + message['role'] + ' ' + message['content'] + '<|im_end|>' + ' '}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant ' }}{% endif %}"
    }

    enum class ThinkingMode {
        DEFAULT,
        DISABLED;

        internal val disableReasoning: Boolean
            get() = this == DISABLED

        internal val reasoningBudget: Int
            get() = if (this == DISABLED) 0 else DEFAULT_REASONING_BUDGET
    }

    /**
     * Data class to hold the inference parameters for the LLM.
     *
     * @property minP The minimum probability for a token to be considered.
     * ```
     *                Also known as top-P sampling. (Default: 0.1f)
     * @property temperature
     * ```
     * The temperature for sampling. Higher values make the output more random.
     * ```
     *                       (Default: 0.8f)
     * @property storeChats
     * ```
     * Whether to store the chat history in memory. If true, the LLM will
     * ```
     *                      remember previous interactions in the current session. (Default: true)
     * @property contextSize
     * ```
     * The context size (in tokens) for the LLM. This determines how much
     * ```
     *                       of the previous conversation the LLM can "remember". If null, the
     *                       value from the GGUF model file will be used, or a default value if
     *                       not present in the model file. (Default: null)
     * @property chatTemplate
     * ```
     * The chat template to use for formatting the conversation. This
     * ```
     *                        is a Jinja2 template string. If null, the value from the GGUF
     *                        model file will be used, or a default value if not present in the
     *                        model file. (Default: null)
     * @property numThreads
     * ```
     * The number of threads to use for inference. (Default: 4)
     * @property useMmap Whether to use memory-mapped file I/O for loading the model.
     * ```
     *                   This can improve loading times and reduce memory usage. (Default: true)
     * @property useMlock
     * ```
     * Whether to lock the model in memory. This can prevent the model from
     * ```
     *                    being swapped out to disk, potentially improving performance. (Default: false)
     * @property thinkingMode
     * ```
     * Controls whether reasoning “think” traces remain enabled. Use
     * ```
     *                        [ThinkingMode.DISABLED] to request the equivalent of llama.cpp's
     *                        `--no-think` flag. (Default: [ThinkingMode.DEFAULT])
     * @property reasoningBudget
     * ```
     * Optional override for llama.cpp's `--reasoning-budget` flag. Set to
     * ```
     *                           `0` to disable thinking explicitly, `-1` to leave it unrestricted,
     *                           or omit to let [thinkingMode] decide.
     * ```
     */
    data class InferenceParams(
            val minP: Float = 0.1f,
            val temperature: Float = 0.8f,
            val storeChats: Boolean = true,
            val contextSize: Long? = null,
            val chatTemplate: String? = null,
            val numThreads: Int = 4,
            val useMmap: Boolean = true,
            val useMlock: Boolean = false,
            val thinkingMode: ThinkingMode = ThinkingMode.DEFAULT,
            val reasoningBudget: Int? = null,
    )

    /**
     * Summary of the most recent response generation.
     *
     * @property tokensPerSecond Average decoding throughput for the response.
     * @property tokenCount Number of tokens emitted for the response.
     * @property elapsedMicros Total decoding time in microseconds.
     */
    data class GenerationMetrics(
            val tokensPerSecond: Float,
            val tokenCount: Long,
            val elapsedMicros: Long,
    ) {
        val elapsedMillis: Double
            get() = elapsedMicros / 1_000.0

        val elapsedSeconds: Double
            get() = elapsedMicros / 1_000_000.0
    }

    /**
     * Loads the GGUF model from the given path. This function will read the metadata from the GGUF
     * model file, such as the context size and chat template, and use them if they are not
     * explicitly provided in the `params`.
     *
     * @param modelPath The path to the GGUF model file.
     * @param params The inference parameters to use. If not provided, default values will be used.
     * ```
     *               If `contextSize` or `chatTemplate` are not provided in `params`,
     *               the values from the GGUF model file will be used. If those are also
     *               not available in the model file, then default values from [DefaultInferenceParams]
     *               will be used.
     * @return
     * ```
     * `true` if the model was loaded successfully, `false` otherwise.
     * @throws FileNotFoundException if the model file is not found at the given path.
     */
    suspend fun load(
            modelPath: String,
            params: InferenceParams = InferenceParams(),
    ) =
            withContext(Dispatchers.IO) {
                if (nativePtr != 0L) {
                    close()
                }

                val ggufReader = GGUFReader()
                val resolvedContextSize: Long
                val resolvedChatTemplate: String
                try {
                    ggufReader.load(modelPath)
                    val modelContextSize =
                            ggufReader.getContextSize() ?: DefaultInferenceParams.contextSize
                    resolvedContextSize = resolveContextSize(params.contextSize, modelContextSize)
                    resolvedChatTemplate = resolveChatTemplate(params.chatTemplate, ggufReader)
                } finally {
                    ggufReader.close()
                }
                nativePtr =
                        nativeBridge.loadModel(
                                this@SmolLM,
                                modelPath,
                                params.minP,
                                params.temperature,
                                params.storeChats,
                                resolvedContextSize,
                                resolvedChatTemplate,
                                params.numThreads,
                                params.useMmap,
                                params.useMlock,
                                useVulkanGPU
                        )
                val reasoningBudget =
                        resolvedReasoningBudget(params.thinkingMode, params.reasoningBudget)
                applyReasoningState(params.thinkingMode, reasoningBudget)
            }

    /**
     * Downloads a GGUF model from Hugging Face (if needed) and loads it for inference.
     *
     * @param context Android context used to resolve the destination directory under app storage.
     * @param modelId Hugging Face repository id (for example, "unsloth/Qwen3-0.6B-GGUF").
     * @param revision Repository revision or branch name. Defaults to "main".
     * @param preferredQuantizations Ordered list of substrings used to pick the desired GGUF
     * variant.
     * @param filename Optional explicit file name/path (relative to the repo root) to download.
     * @param params Inference parameters to apply once the model is loaded.
     * @param token Optional Hugging Face access token for private repositories.
     * @param forceDownload When true, always redownload the file even if a cached copy exists.
     * @param preferSystemDownloader When true, prefer Android's DownloadManager for large
     * downloads.
     * @param onProgress Optional progress listener receiving downloaded bytes and total bytes (when
     * known).
     *
     * @return [HuggingFaceHub.ModelDownloadResult] describing the loaded asset.
     */
    suspend fun loadFromHuggingFace(
            context: Context,
            modelId: String,
            revision: String = "main",
            preferredQuantizations: List<String> = HuggingFaceHub.DEFAULT_QUANTIZATION_PRIORITIES,
            filename: String? = null,
            params: InferenceParams = InferenceParams(),
            token: String? = null,
            forceDownload: Boolean = false,
            preferSystemDownloader: Boolean = true,
            onProgress: ((downloaded: Long, total: Long?) -> Unit)? = null,
    ): HuggingFaceHub.ModelDownloadResult {
        val downloadResult =
                HuggingFaceHub.ensureModelOnDisk(
                        context = context,
                        modelId = modelId,
                        revision = revision,
                        preferredQuantizations = preferredQuantizations,
                        filename = filename,
                        token = token,
                        forceDownload = forceDownload,
                        preferSystemDownloader = preferSystemDownloader,
                        onProgress = onProgress,
                )
        load(downloadResult.file.absolutePath, params)
        return downloadResult
    }

    /**
     * Adds a user message to the chat history. This message will be considered as part of the
     * conversation when generating the next response.
     *
     * @param message The user's message.
     * @throws IllegalStateException if the model is not loaded.
     */
    fun addUserMessage(message: String) {
        verifyHandle()
        nativeBridge.addChatMessage(this, nativePtr, message, "user")
    }

    /** Adds the system prompt for the LLM */
    fun addSystemPrompt(prompt: String) {
        verifyHandle()
        nativeBridge.addChatMessage(this, nativePtr, prompt, "system")
    }

    /**
     * Adds the assistant message for LLM inference An assistant message is the response given by
     * the LLM for a previous query in the conversation
     */
    fun addAssistantMessage(message: String) {
        verifyHandle()
        nativeBridge.addChatMessage(this, nativePtr, message, "assistant")
    }

    fun getThinkingMode(): ThinkingMode = currentThinkingMode

    fun getReasoningBudget(): Int = currentReasoningBudget

    fun isThinkingEnabled(): Boolean = currentReasoningBudget != 0

    fun setThinkingMode(mode: ThinkingMode) {
        verifyHandle()
        val targetBudget = if (mode.disableReasoning) 0 else DEFAULT_REASONING_BUDGET
        applyReasoningState(mode, targetBudget)
    }

    fun setThinkingEnabled(enabled: Boolean) {
        setThinkingMode(if (enabled) ThinkingMode.DEFAULT else ThinkingMode.DISABLED)
    }

    fun setReasoningBudget(budget: Int) {
        verifyHandle()
        val mode = if (budget == 0) ThinkingMode.DISABLED else ThinkingMode.DEFAULT
        applyReasoningState(mode, budget)
    }

    /**
     * Returns the rate (in tokens per second) at which the LLM generated its last response via
     * `getResponse()`
     */
    fun getResponseGenerationSpeed(): Float {
        verifyHandle()
        return nativeBridge.getResponseGenerationSpeed(this, nativePtr)
    }

    /**
     * Returns throughput information for the last completed response. The metrics are reset on the
     * next call to [getResponse] or [getResponseAsFlow].
     */
    fun getLastGenerationMetrics(): GenerationMetrics {
        verifyHandle()
        val elapsedMicros = nativeBridge.getResponseGenerationDurationMicros(this, nativePtr)
        val tokenCount = nativeBridge.getResponseGeneratedTokenCount(this, nativePtr)
        val tokensPerSecond =
                if (elapsedMicros <= 0L || tokenCount <= 0L) {
                    0f
                } else {
                    nativeBridge.getResponseGenerationSpeed(this, nativePtr)
                }
        return GenerationMetrics(
                tokensPerSecond = tokensPerSecond,
                tokenCount = tokenCount,
                elapsedMicros = elapsedMicros
        )
    }

    /**
     * Returns the number of tokens consumed by the LLM's context window The context of the LLM is
     * roughly the output of, tokenize(apply_chat_template(messages_in_conversation))
     */
    fun getContextLengthUsed(): Int {
        verifyHandle()
        return nativeBridge.getContextSizeUsed(this, nativePtr)
    }

    /**
     * Return the LLM response to the given query as an async Flow. This is useful for streaming the
     * response as it is generated by the LLM.
     *
     * @param query The query to ask the LLM.
     * @return A Flow of Strings, where each String is a piece of the response.
     * ```
     *         The flow completes when the LLM has finished generating the response.
     *         The special token "[EOG]" (End Of Generation) indicates the end of the response.
     * @throws IllegalStateException
     * ```
     * if the model is not loaded.
     */
    fun getResponseAsFlow(query: String): Flow<String> =
            flow {
                        verifyHandle()
                        nativeBridge.startCompletion(this@SmolLM, nativePtr, query)
                        var piece = nativeBridge.completionLoop(this@SmolLM, nativePtr)
                        while (piece != "[EOG]") {
                            emit(piece) // Emit immediately for fastest TTFT
                            piece = nativeBridge.completionLoop(this@SmolLM, nativePtr)
                        }
                        nativeBridge.stopCompletion(this@SmolLM, nativePtr)
                    }
                    .flowOn(Dispatchers.IO) // Run on IO dispatcher for better performance

    /**
     * Returns the LLM response to the given query as a String. This function is blocking and will
     * return the complete response.
     *
     * @param query The user's query/prompt for the LLM.
     * @param maxTokens Maximum number of tokens to generate. -1 for infinite (until EOS).
     * @return The complete response from the LLM.
     * @throws IllegalStateException if the model is not loaded.
     */
    fun getResponse(query: String, maxTokens: Int = -1): String {
        verifyHandle()
        Log.d(LOG_TAG, "getResponse: starting completion. maxTokens=$maxTokens, queryLength=${query.length}")
        nativeBridge.startCompletion(this@SmolLM, nativePtr, query)
        var piece = nativeBridge.completionLoop(this@SmolLM, nativePtr)
        var response = ""
        var tokensGenerated = 0
        
        while (piece != "[EOG]") {
            response += piece
            tokensGenerated++
            
            if (tokensGenerated % 10 == 0) {
                 // Log occasional progress to confirm it's alive without spamming
                 Log.d(LOG_TAG, "Generated $tokensGenerated tokens...")
            }

            if (maxTokens > 0 && tokensGenerated >= maxTokens) {
                Log.d(LOG_TAG, "getResponse: maxTokens ($maxTokens) reached. Stopping.")
                break
            }
            
            piece = nativeBridge.completionLoop(this@SmolLM, nativePtr)
        }
        if (piece == "[EOG]") {
             Log.d(LOG_TAG, "getResponse: [EOG] received after $tokensGenerated tokens.")
        }
        
        nativeBridge.stopCompletion(this, nativePtr)
        Log.d(LOG_TAG, "getResponse: finished. Total length=${response.length}")
        return response
    }

    /**
     * Unloads the LLM model and releases resources. This method should be called when the SmolLM
     * instance is no longer needed to prevent memory leaks.
     */
    override fun close() {
        if (nativePtr != 0L) {
            nativeBridge.close(this, nativePtr)
            nativePtr = 0L
        }
        currentThinkingMode = ThinkingMode.DEFAULT
        currentReasoningBudget = DEFAULT_REASONING_BUDGET
    }

    private fun verifyHandle() {
        assert(nativePtr != 0L) { "Model is not loaded. Use SmolLM.create to load the model" }
    }

    private external fun loadModel(
            modelPath: String,
            minP: Float,
            temperature: Float,
            storeChats: Boolean,
            contextSize: Long,
            chatTemplate: String,
            nThreads: Int,
            useMmap: Boolean,
            useMlock: Boolean,
            useVulkan: Boolean
    ): Long

    private external fun setReasoningOptions(
            modelPtr: Long,
            disableThinking: Boolean,
            reasoningBudget: Int,
    )

    private external fun addChatMessage(
            modelPtr: Long,
            message: String,
            role: String,
    )

    private external fun getResponseGenerationSpeed(modelPtr: Long): Float

    private external fun getResponseGeneratedTokenCount(modelPtr: Long): Long

    private external fun getResponseGenerationDurationMicros(modelPtr: Long): Long

    private external fun getContextSizeUsed(modelPtr: Long): Int

    // Return native llama_model* pointer for advanced native integrations (do not free)
    private external fun getNativeModelPtr(modelPtr: Long): Long

    /**
     * Public helper to return the underlying native llama_model* pointer. This is intended for
     * advanced integrations (e.g., native projector) and should NOT be used to free or modify the
     * native model directly.
     */
    fun getNativeModelPointer(): Long {
        verifyHandle()
        return nativeBridge.getNativeModelPtr(this, nativePtr)
    }

    // Decode embeddings prepared by the projector (raw floats) without loading mmproj
    private external fun nativeDecodePreparedEmbeddings(
            modelPtr: Long,
            embdPath: String,
            metaPath: String,
            nBatch: Int
    ): Boolean

    /**
     * Decode prepared embeddings previously produced by Projector.encodeImageToFile. This will
     * replay the required llama.decode steps using the current loaded model/context so the image
     * embeddings are present in the KV cache for subsequent generation. Returns true on success.
     */
    fun decodePreparedEmbeddings(embdPath: String, metaPath: String, nBatch: Int = 1): Boolean {
        verifyHandle()
        return try {
            nativeBridge.nativeDecodePreparedEmbeddings(this, nativePtr, embdPath, metaPath, nBatch)
        } catch (e: UnsatisfiedLinkError) {
            false
        }
    }

    private external fun close(modelPtr: Long)

    private external fun startCompletion(
            modelPtr: Long,
            prompt: String,
    )

    private external fun completionLoop(modelPtr: Long): String

    private external fun stopCompletion(modelPtr: Long)

    private fun applyReasoningState(mode: ThinkingMode, budget: Int) {
        val effectiveMode = if (budget == 0) ThinkingMode.DISABLED else mode
        currentThinkingMode = effectiveMode
        currentReasoningBudget = budget
        if (nativePtr != 0L) {
            nativeBridge.setReasoningOptions(
                    this,
                    nativePtr,
                    effectiveMode.disableReasoning || budget == 0,
                    budget
            )
        }
    }

    private fun resolvedReasoningBudget(mode: ThinkingMode, override: Int?): Int {
        return override ?: if (mode.disableReasoning) 0 else DEFAULT_REASONING_BUDGET
    }

    private fun resolveContextSize(requested: Long?, modelContextSize: Long): Long {
        if (requested != null) {
            // If explicitly requested, trust the caller and clamp only to absolute limits
            return requested.coerceIn(MIN_CONTEXT_SIZE, DEFAULT_CONTEXT_SIZE_CAP)
        }
        val desired = modelContextSize
        val heapAwareCap = recommendedContextCap()
        val effectiveCap = minOf(DEFAULT_CONTEXT_SIZE_CAP, heapAwareCap)
        val clamped = desired.coerceIn(MIN_CONTEXT_SIZE, effectiveCap)
        if (desired != clamped) {
            val heapMb = Runtime.getRuntime().maxMemory() / (1024 * 1024)
            Log.w(
                    LOG_TAG,
                    "Context window $desired→$clamped tokens to fit heap (${heapMb}MB max). " +
                            "Override via InferenceParams(contextSize=...).",
            )
        }
        return clamped
    }

    private fun resolveChatTemplate(explicit: String?, ggufReader: GGUFReader): String =
            explicit ?: (ggufReader.getChatTemplate() ?: DefaultInferenceParams.chatTemplate)

    private fun recommendedContextCap(): Long {
        val heapMb = Runtime.getRuntime().maxMemory() / (1024 * 1024)
        return when {
            heapMb <= 256 -> 2_048L
            heapMb <= 384 -> 4_096L
            heapMb <= 512 -> 6_144L
            else -> DEFAULT_CONTEXT_SIZE_CAP
        }
    }
}
